{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10369298,"sourceType":"datasetVersion","datasetId":6422718}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade bitsandbytes\n!pip install --upgrade datasets tokenizers\n!pip install --upgrade transformers","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-01-05T01:57:27.935373Z","iopub.execute_input":"2025-01-05T01:57:27.935621Z","iopub.status.idle":"2025-01-05T01:57:53.446610Z","shell.execute_reply.started":"2025-01-05T01:57:27.935592Z","shell.execute_reply":"2025-01-05T01:57:53.445699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom transformers import LlavaForConditionalGeneration\nfrom transformers import AutoProcessor\nfrom transformers import BitsAndBytesConfig\nfrom transformers import GenerationConfig\n\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-05T01:57:53.450576Z","iopub.execute_input":"2025-01-05T01:57:53.450844Z","iopub.status.idle":"2025-01-05T01:58:09.119087Z","shell.execute_reply.started":"2025-01-05T01:57:53.450813Z","shell.execute_reply":"2025-01-05T01:58:09.118434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = open('/kaggle/input/coco-vqa-dataset/vaq2.0.TrainImages.txt', 'r')\nlines = data.readlines()\nprint(lines[:5])  # Print the first 5 lines","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load train data\ntrain_data = []\ntrain_path = '/kaggle/input/coco-vqa-dataset/vaq2.0.TrainImages.txt'\nwith open(train_path, 'r') as f:\n    for i, line in enumerate(f.readlines()):\n        \n        full_sentence = line.split('\\t')\n        if (i < 3):\n            print(\"Full sentence: \", full_sentence)\n        \n        img_path = full_sentence[0][:-2]\n        if (i < 3):\n            print(\"Image Path: \", img_path)\n        \n        qa = full_sentence[1].split('?')\n        \n        question = qa[0]\n        if (i < 3):\n            print(\"Question: \", question)\n\n        # Error handling in case\n        if len(qa) == 3:\n            answer = qa[2]\n        else:\n            answer = qa[1]\n        \n        # Remove any trailing newline characters or extra spaces from the answer\n        answer = answer.strip()\n        \n        if (i < 3):\n            print(\"Answer: \", answer)\n            \n        if (i < 3):\n            print(\" \")\n            \n        data_sample = {\n            'Image Path': img_path,\n            'Question': question + '?',\n            'Answer': answer  # No trailing newline\n        }\n        train_data.append(data_sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T01:42:50.196483Z","iopub.execute_input":"2025-01-05T01:42:50.196889Z","iopub.status.idle":"2025-01-05T01:42:50.225576Z","shell.execute_reply.started":"2025-01-05T01:42:50.196862Z","shell.execute_reply":"2025-01-05T01:42:50.224946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define quantization configuration\nquantization_config = BitsAndBytesConfig(load_in_4bit=True, \n                                          bnb_4bit_compute_dtype=torch.float16)\n\n# Define model ID\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\n\n# Set device (cuda if available, else cpu)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load processor (e.g., tokenizer, feature extractor, etc.)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n# Correct way to load the model with the quantization config and device map as keyword arguments\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id,\n    quantization_config=quantization_config,  # Pass as keyword argument \n    torch_dtype=torch.float16                 # Set dtype to float16 (optional, but often used with quantization)\n)\n\n# Move the model to the specified device\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T01:43:08.653980Z","iopub.execute_input":"2025-01-05T01:43:08.654287Z","iopub.status.idle":"2025-01-05T01:43:08.664978Z","shell.execute_reply.started":"2025-01-05T01:43:08.654266Z","shell.execute_reply":"2025-01-05T01:43:08.663905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Guide said don't change unless know Prompt Engineering (I don't know wtf that is)\ndef create_prompt(question):\n    prompt = f\"\"\" ### INSTRUCTION:\nYour task is to answer the question based on the given image. You can only answer 'yes' or 'not'.\n### USER: <image>\n{question}\n### ASSISTANT:\"\"\"\n    return prompt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generation_config = GenerationConfig(\n    max_new_tokens=10,\n    do_sample=True,\n    temperature=0.1,\n    top_p=0.95,\n    top_k = 50,\n    eos_token_id = model.config.eos_token_id,\n    pad_token=model.config.pad_token_id\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 0\nquestion = train_data[idx]['Question']\nimg = train_data[idx]['Image Path']\nimg_path = os.path.join('/kaggle/input/coco-vqa-dataset/val2014-resised', img)\nlabel = train_data[idx]['Answer']\nimage = Image.open(img_path)\n\nprompt = create_prompt(question)\ninputs = processor(prompt,\n                  image,\n                  padding = True,\n                  return_tensors = 'pt').to(device)\n\noutput = model.generate(**inputs, generation_config=generation_config)\ngenerated_text = processor.decode(output[0], skip_special_tokens = True)\n\nplt.imshow(image)\nplt.axis(\"off\")\nplt.show()\nprint(f\"Question: {question}\")\nprint(f\"Label: {label}\")\nprint(f\"Prediction: {generated_text.split('### ASSISTANT: ')[-1]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}