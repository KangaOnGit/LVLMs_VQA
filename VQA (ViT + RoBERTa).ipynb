{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10367779,"sourceType":"datasetVersion","datasetId":6421612}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random as rand\nimport os\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms\nfrom transformers import ViTModel, ViTImageProcessor\nfrom transformers import AutoTokenizer, RobertaModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:31:28.417514Z","iopub.execute_input":"2025-01-04T11:31:28.417752Z","iopub.status.idle":"2025-01-04T11:31:43.664337Z","shell.execute_reply.started":"2025-01-04T11:31:28.417703Z","shell.execute_reply":"2025-01-04T11:31:43.663651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed):\n  rand.seed(seed)\n  np.random.seed(seed)\n  torch.cuda.manual_seed(seed)\n  torch.cuda.manual_seed_all(seed)\n  torch.backends.cudnn.deterministic = True\n  torch.backends.cudnn.benchmark = False\n\nseed = 59\nset_seed(59)\nprint(\"Code ran successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:31:58.531943Z","iopub.execute_input":"2025-01-04T11:31:58.532217Z","iopub.status.idle":"2025-01-04T11:31:58.538743Z","shell.execute_reply.started":"2025-01-04T11:31:58.532195Z","shell.execute_reply":"2025-01-04T11:31:58.537803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = open('/kaggle/input/vqa-set/vaq2.0.TrainImages.txt', 'r')\nlines = data.readlines()\nprint(lines[:5])  # Print the first 5 lines","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:01.004128Z","iopub.execute_input":"2025-01-04T11:32:01.004442Z","iopub.status.idle":"2025-01-04T11:32:01.023626Z","shell.execute_reply.started":"2025-01-04T11:32:01.004413Z","shell.execute_reply":"2025-01-04T11:32:01.022798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load train data\ntrain_data = []\ntrain_path = '/kaggle/input/vqa-set/vaq2.0.TrainImages.txt'\nwith open(train_path, 'r') as f:\n    for i, line in enumerate(f.readlines()):\n        \n        full_sentence = line.split('\\t')\n        if (i < 3):\n            print(\"Full sentence: \", full_sentence)\n        \n        img_path = full_sentence[0][:-2]\n        if (i < 3):\n            print(\"Image Path: \", img_path)\n        \n        qa = full_sentence[1].split('?')\n        \n        question = qa[0]\n        if (i < 3):\n            print(\"Question: \", question)\n\n        # Error handling in case\n        if len(qa) == 3:\n            answer = qa[2]\n        else:\n            answer = qa[1]\n        \n        # Remove any trailing newline characters or extra spaces from the answer\n        answer = answer.strip()\n        \n        if (i < 3):\n            print(\"Answer: \", answer)\n            \n        if (i < 3):\n            print(\" \")\n            \n        data_sample = {\n            'Image Path': img_path,\n            'Question': question + '?',\n            'Answer': answer  # No trailing newline\n        }\n        train_data.append(data_sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:02.995604Z","iopub.execute_input":"2025-01-04T11:32:02.995947Z","iopub.status.idle":"2025-01-04T11:32:03.024450Z","shell.execute_reply.started":"2025-01-04T11:32:02.995917Z","shell.execute_reply":"2025-01-04T11:32:03.023575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load val data\nval_data = []\nval_path = '/kaggle/input/vqa-set/vaq2.0.DevImages.txt'\nwith open(val_path, 'r') as f:\n    for i, line in enumerate(f.readlines()):\n        \n        full_sentence = line.split('\\t')\n        if (i < 3):\n            print(\"Full sentence: \", full_sentence)\n        \n        img_path = full_sentence[0][:-2]\n        if (i < 3):\n            print(\"Image Path: \", img_path)\n        \n        qa = full_sentence[1].split('?')\n        \n        question = qa[0]\n        if (i < 3):\n            print(\"Question: \", question)\n\n        # Error handling in case\n        if len(qa) == 3:\n            answer = qa[2]\n        else:\n            answer = qa[1]\n        \n        # Remove any trailing newline characters or extra spaces from the answer\n        answer = answer.strip()\n        \n        if (i < 3):\n            print(\"Answer: \", answer)\n            \n        if (i < 3):\n            print(\" \")\n            \n        data_sample = {\n            'Image Path': img_path,\n            'Question': question + '?',\n            'Answer': answer  # No trailing newline\n        }\n        val_data.append(data_sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:05.611085Z","iopub.execute_input":"2025-01-04T11:32:05.611412Z","iopub.status.idle":"2025-01-04T11:32:05.635984Z","shell.execute_reply.started":"2025-01-04T11:32:05.611384Z","shell.execute_reply":"2025-01-04T11:32:05.635187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load train data\ntest_data = []\ntest_path = '/kaggle/input/vqa-set/vaq2.0.TestImages.txt'\nwith open(test_path, 'r') as f:\n    for i, line in enumerate(f.readlines()):\n        \n        full_sentence = line.split('\\t')\n        if (i < 3):\n            print(\"Full sentence: \", full_sentence)\n        \n        img_path = full_sentence[0][:-2]\n        if (i < 3):\n            print(\"Image Path: \", img_path)\n        \n        qa = full_sentence[1].split('?')\n        \n        question = qa[0]\n        if (i < 3):\n            print(\"Question: \", question)\n\n        # Error handling in case\n        if len(qa) == 3:\n            answer = qa[2]\n        else:\n            answer = qa[1]\n        \n        # Remove any trailing newline characters or extra spaces from the answer\n        answer = answer.strip()\n        \n        if (i < 3):\n            print(\"Answer: \", answer)\n            \n        if (i < 3):\n            print(\" \")\n            \n        data_sample = {\n            'Image Path': img_path,\n            'Question': question + '?',\n            'Answer': answer  # No trailing newline\n        }\n        test_data.append(data_sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:11.066617Z","iopub.execute_input":"2025-01-04T11:32:11.066937Z","iopub.status.idle":"2025-01-04T11:32:11.091824Z","shell.execute_reply.started":"2025-01-04T11:32:11.066910Z","shell.execute_reply":"2025-01-04T11:32:11.090315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get all classes\nclasses = set([sample['Answer'] for sample in train_data])\n\nlabel2idx = {\n    cls_name: idx for idx, cls_name in enumerate(classes)\n}\nprint(label2idx)\nprint(\"Keys: \", label2idx.keys())\nprint(\"Values: \", label2idx.values())\n\nprint(\"\")\n\nidx2label = {\n    idx: cls_name for idx, cls_name in enumerate(classes)\n}\nprint(idx2label)\nprint(\"Keys: \", idx2label.keys())\nprint(\"Values: \", idx2label.values())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:13.449684Z","iopub.execute_input":"2025-01-04T11:32:13.450030Z","iopub.status.idle":"2025-01-04T11:32:13.458325Z","shell.execute_reply.started":"2025-01-04T11:32:13.450003Z","shell.execute_reply":"2025-01-04T11:32:13.457474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VQA_Dataset(Dataset):\n    def __init__(self, data, label2idx, img_feature_extractor, text_tokenizer,\n                device, transforms = None,\n                img_dir = '/kaggle/input/vqa-set/val2014-resised'):\n        self.data = data\n        self.img_dir = img_dir\n        self.label2idx = label2idx\n        self.img_encoder = img_feature_extractor\n        self.text_tokenizer = text_tokenizer\n        self.device = device\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.data[idx]['Image Path'])\n        img = Image.open(img_path).convert('RGB')\n\n        if self.transforms:\n            img = self.transforms(img)\n\n        if self.img_encoder: # Image Encoder\n            img = self.img_encoder(images = img, return_tensors = 'pt')\n            img = {k: v.to(self.device).squeeze(0) for k, v in img.items()}\n        question = self.data[idx]['Question']\n        \n        if self.text_tokenizer: # Text Encoder\n            question = self.text_tokenizer(\n                question,\n                padding = \"max_length\",\n                max_length = 20,\n                truncation = True,\n                return_tensors = 'pt'\n            )\n            question = {k: v.to(self.device).squeeze(0) for k, v in question.items()}\n\n        label = self.data[idx]['Answer']\n        label = torch.tensor(\n            self.label2idx[label],\n            dtype = torch.long\n        ).to(self.device)\n        \n        # The Encoded Image and Question with Label (Binary)\n        sample = {\n            'image': img,\n            'question': question,\n            'label': label\n        }\n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:15.190175Z","iopub.execute_input":"2025-01-04T11:32:15.190462Z","iopub.status.idle":"2025-01-04T11:32:15.197613Z","shell.execute_reply.started":"2025-01-04T11:32:15.190439Z","shell.execute_reply":"2025-01-04T11:32:15.196944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_transform = transforms.Compose([\n    transforms.Resize(size = (224, 224)),\n    transforms.CenterCrop(size = 180),\n    transforms.ColorJitter(brightness = 0.1, contrast = 0.1, saturation = 0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.GaussianBlur(3),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:17.387413Z","iopub.execute_input":"2025-01-04T11:32:17.387697Z","iopub.status.idle":"2025-01-04T11:32:17.392397Z","shell.execute_reply.started":"2025-01-04T11:32:17.387674Z","shell.execute_reply":"2025-01-04T11:32:17.391450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_encoder = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\ntext_encoder = AutoTokenizer.from_pretrained('roberta-base')\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntrain_set = VQA_Dataset(\n    train_data,\n    label2idx = label2idx,\n    img_feature_extractor = img_encoder,\n    text_tokenizer = text_encoder,\n    device = device,\n    transforms = data_transform\n)\n\nval_set = VQA_Dataset(\n    val_data,\n    label2idx = label2idx,\n    img_feature_extractor = img_encoder,\n    text_tokenizer = text_encoder,\n    device = device\n)\n\n\ntest_set = VQA_Dataset(\n    test_data,\n    label2idx = label2idx,\n    img_feature_extractor = img_encoder,\n    text_tokenizer = text_encoder,\n    device = device\n)\n\nprint(train_set)\nprint(val_set)\nprint(test_set)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:19.709876Z","iopub.execute_input":"2025-01-04T11:32:19.710186Z","iopub.status.idle":"2025-01-04T11:32:22.002509Z","shell.execute_reply.started":"2025-01-04T11:32:19.710156Z","shell.execute_reply":"2025-01-04T11:32:22.001557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_batch = 256\ntest_batch = 32\n\ntrain_loader = DataLoader(\n    train_set,\n    batch_size = train_batch,\n    shuffle = True\n)\n\nval_loader = DataLoader(\n    val_set,\n    batch_size = test_batch,\n    shuffle = False\n)\n\ntest_loader = DataLoader(\n    test_set,\n    batch_size = test_batch,\n    shuffle = False\n)\nprint(train_loader)\nprint(val_loader)\nprint(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:23.449997Z","iopub.execute_input":"2025-01-04T11:32:23.450287Z","iopub.status.idle":"2025-01-04T11:32:23.456248Z","shell.execute_reply.started":"2025-01-04T11:32:23.450260Z","shell.execute_reply":"2025-01-04T11:32:23.455543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self):\n        super(TextEncoder, self).__init__()\n        self.model = RobertaModel.from_pretrained('roberta-base')\n\n    def forward(self, x):\n        out = self.model(**x)\n        return out.pooler_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:25.241181Z","iopub.execute_input":"2025-01-04T11:32:25.241612Z","iopub.status.idle":"2025-01-04T11:32:25.247177Z","shell.execute_reply.started":"2025-01-04T11:32:25.241576Z","shell.execute_reply":"2025-01-04T11:32:25.246247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VisualEncoder(nn.Module):\n    def __init__(self):\n        super(VisualEncoder, self).__init__()\n        self.model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n\n    def forward(self, x):\n        out = self.model(**x)\n        return out.pooler_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:26.857361Z","iopub.execute_input":"2025-01-04T11:32:26.857676Z","iopub.status.idle":"2025-01-04T11:32:26.861801Z","shell.execute_reply.started":"2025-01-04T11:32:26.857647Z","shell.execute_reply":"2025-01-04T11:32:26.861083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, hidden_size=512, dropout_prob = .2, n_classes = 2):\n        super(Classifier, self).__init__()\n        self.fc1 = nn.Linear(768*2, hidden_size)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.gelu = nn.GELU()\n        self.fc2 = nn.Linear(hidden_size, n_classes)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:28.490217Z","iopub.execute_input":"2025-01-04T11:32:28.490493Z","iopub.status.idle":"2025-01-04T11:32:28.495671Z","shell.execute_reply.started":"2025-01-04T11:32:28.490471Z","shell.execute_reply":"2025-01-04T11:32:28.494660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VQA_Model(nn.Module):\n    def __init__(self, visual_encoder, text_encoder, classifier):\n        super(VQA_Model, self).__init__()\n        self.visual_encoder = visual_encoder\n        self.text_encoder = text_encoder\n        self.classifier = classifier\n\n    def forward(self, img, ans):\n        text_out = self.text_encoder(ans)\n        img_out = self.visual_encoder(img)\n        x = torch.concat((img_out, text_out), dim  =1)\n        x = self.classifier(x)\n        return x\n    \n    def freeze(self, visual=True, textual=True, classifier = False):\n        if visual:\n            for n, p in self.visual_encoder.named_parameters():\n                p.requires_grad = False\n\n        if textual:\n            for n, p in self.visual_encoder.named_parameters():\n                p.requires_grad = False\n\n        if classifier:\n            for n, p in self.visual_encoder.named_parameters():\n                p.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:30.178118Z","iopub.execute_input":"2025-01-04T11:32:30.178399Z","iopub.status.idle":"2025-01-04T11:32:30.183773Z","shell.execute_reply.started":"2025-01-04T11:32:30.178376Z","shell.execute_reply":"2025-01-04T11:32:30.183137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_classes = len(classes)\nhidden_size = 256\ndropout_prob = 0.2\ntext_encoder = TextEncoder().to(device)\nvisual_encoder = VisualEncoder().to(device)\nclassifier = Classifier(hidden_size, dropout_prob, n_classes).to(device)\nmodel = VQA_Model(visual_encoder, text_encoder, classifier).to(device)\nmodel.freeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:32.113100Z","iopub.execute_input":"2025-01-04T11:32:32.113508Z","iopub.status.idle":"2025-01-04T11:32:36.816659Z","shell.execute_reply.started":"2025-01-04T11:32:32.113461Z","shell.execute_reply":"2025-01-04T11:32:36.815982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval(model, val_set, criterion, device):\n    model.eval()\n    corr = 0\n    total = 0\n    losses = []\n\n    with torch.no_grad():\n        for idx, inputs in enumerate(val_set):\n            \n            # Move inputs to device\n            img = inputs['image']\n            labels = inputs['label']\n\n            # Handle optional 'questions' key\n            questions = inputs['question']\n\n            # Forward pass\n            if questions is not None:\n                outputs = model(img, questions)\n            else:\n                outputs = model(img)\n\n            # Calculate loss\n            loss = criterion(outputs, labels)\n            losses.append(loss.item())\n\n            # Calculate accuracy\n            _, pred = torch.max(outputs.data, 1)  # Index of max logits\n            total += labels.size(0)\n            corr += (pred == labels).sum().item()\n\n    # Calculate average loss and accuracy\n    loss = sum(losses) / len(losses) if losses else 0.0\n    acc = corr / total if total > 0 else 0.0\n\n    return loss, acc\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:32:38.014324Z","iopub.execute_input":"2025-01-04T11:32:38.014616Z","iopub.status.idle":"2025-01-04T11:32:38.021679Z","shell.execute_reply.started":"2025-01-04T11:32:38.014592Z","shell.execute_reply":"2025-01-04T11:32:38.020643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\ndef train(model, train_data, val_data, criterion, optim, scheduler, device, epochs, log_interval = 10):\n    train_loss = []\n    val_loss = []\n    for i in range(epochs):\n        print(\"-\" * 59)\n        print(f\"Starting Epoch {i + 1}...\")\n        epoch_start_time = time.time()\n\n        batch_loss = []\n        model.train()\n        \n        for idx, inputs in enumerate(train_data):\n            image = inputs['image']\n            questions = inputs['question']\n            labels = inputs['label']\n\n            optim.zero_grad()\n\n            out = model(image, questions)\n            loss = criterion(out, labels)\n            loss.backward()\n            optim.step()\n            batch_loss.append(loss.item())\n            if idx % log_interval == 0 and idx > 0:\n                print(f\"| Epoch: {i + 1} | {idx + 1}/{len(train_data)} Batches | Train Loss: {loss.item():.4f} |\")\n            \n        train_loss1 = sum(batch_loss)/len(batch_loss)\n        train_loss.append(train_loss1)\n        val_loss1, val_acc = eval(\n            model, val_data, criterion, device\n        )\n        val_loss.append(val_loss1)\n        \n        print(f\"| Epoch: {i + 1}/{epochs:3d} | Train Loss: {train_loss1:.4f} | Val Loss: {val_loss1:.4f} | Val Accuracy: {val_acc:.2f} | Time: {(time.time() - epoch_start_time):.2f}s |\")\n        print(f\"Epoch {i + 1} was ran successfully\")\n        print(\"-\"*59)\n        \n        scheduler.step()\n    return train_loss, val_loss\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:33:31.978923Z","iopub.execute_input":"2025-01-04T11:33:31.979203Z","iopub.status.idle":"2025-01-04T11:33:31.986970Z","shell.execute_reply.started":"2025-01-04T11:33:31.979180Z","shell.execute_reply":"2025-01-04T11:33:31.986129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr = 1e-3\nepochs = 50\n\nscheduler_step_size = epochs*0.8\ncriterion = nn.CrossEntropyLoss()\noptim = torch.optim.Adam(\n    model.parameters(),\n    lr = lr\n)\n\nscheduler = torch.optim.lr_scheduler.StepLR(\n    optim,\n    step_size = scheduler_step_size,\n    gamma = 0.1\n)\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:33:34.066048Z","iopub.execute_input":"2025-01-04T11:33:34.066328Z","iopub.status.idle":"2025-01-04T11:33:34.073536Z","shell.execute_reply.started":"2025-01-04T11:33:34.066306Z","shell.execute_reply":"2025-01-04T11:33:34.072609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loss, val_loss = train(model,\n                            train_loader,\n                            val_loader,\n                            criterion,\n                            optim,\n                            scheduler,\n                            device,\n                            epochs)\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:33:35.336755Z","iopub.execute_input":"2025-01-04T11:33:35.337025Z","iopub.status.idle":"2025-01-04T11:37:45.345597Z","shell.execute_reply.started":"2025-01-04T11:33:35.337002Z","shell.execute_reply":"2025-01-04T11:37:45.344433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss, val_acc = eval(model,\n                        val_loader,\n                        criterion,\n                        device)\n\ntest_loss, test_acc = eval(\n    model,\n    test_loader,\n    criterion,\n    device\n)\nprint(\"Done!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Val Accuracy: {val_acc}\")\nprint(f\"Test Accuracy: {test_acc}\")\nprint(\"Done!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}